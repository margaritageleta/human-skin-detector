{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the different packages necessary to execute our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "from SkinDetector import SkinDetector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from SkinDetector import SkinDetector\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten, BatchNormalization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize the \"SkinDetector\", our previous phase code, that binarizes images detecting skin pixels and setting them to white while the background is set to black. We then process the binarized images and set them to the same size, 480x640, this is necessary because if we do not resize the images, the model will not train them. \n",
    "We resize them by creating a new black image, the same colour as our background, and them inserting the white pixels corresponding to the detected skin. We do the same with the test images aswell.\n",
    "We process the labels too, by setting them to categorical so the model will work correctly. \n",
    "(The code executes satisfactorily and the warnings are not a problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SkinDetector(dilate=2)\n",
    "train = sd.segment_dataset(sd.TR_DATA)\n",
    "train_data = []\n",
    "for i in range(0,60):\n",
    "    result = np.zeros((480,640))\n",
    "    result[:train[i].shape[0],:train[i].shape[1]] = train[i]\n",
    "    train_data.append(result)\n",
    "train_data = np.asarray(train_data)\n",
    "train_labels = np.asarray(sd.TR_LABEL)\n",
    "\n",
    "test = sd.segment_dataset(sd.VD_DATA)\n",
    "test_data = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    result = np.zeros((480,640))\n",
    "    result[:test[i].shape[0],:test[i].shape[1]] = test[i]\n",
    "    test_data.append(result)\n",
    "\n",
    "test_data = np.asarray(test_data)\n",
    "test_labels = np.asarray(sd.VD_LABEL)\n",
    "\n",
    "train_labels2 = np.zeros(len(sd.TR_LABEL))\n",
    "for it in range (0,len(sd.TR_LABEL)):\n",
    "    train_labels2[it] = sd.TR_LABEL[it]-1\n",
    "\n",
    "test_labels2 = np.zeros(len(sd.VD_LABEL))\n",
    "for it in range (0,len(sd.VD_LABEL)):\n",
    "    test_labels2[it] = sd.VD_LABEL[it]-1\n",
    "\n",
    "train_labels = to_categorical(train_labels2)\n",
    "test_labels = to_categorical(test_labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our model, as we can see this is a Convolutional based model, with just one convolutional layer, some max pooling and two linear layers at the end to classify into the 6 possible classes (0 fingers to 5 fingers).\n",
    "The first convolutional layer is taking 4 arguments, the first is the number of filters, 32, the second one is the size of the filters, 3x3, the the size of the images, in our case 480x640 and the \"1\" to indicate they are binary images. Lastly the activation fucntion, here \"relu\" stands for a recrtifier function.\n",
    "Now we perform a pooling operation on the resultant feature maps we get after the convolution is done on an image. The primary aim of a pooling layers is to reduce the size of the images as much as possible, we are trying to reduce the total number of nodes for the upcoming layers. To do this pooling we take a 2x2 matrix, to minimize pixel loss and get a precise region where the feature are located.\n",
    "The as we can see, we flatten hte pooled images, this is pretty self-explanatory, we just transform the 2D array (pooled images) and converting them to a one dimensional single vector.\n",
    "The next one we can refer to it as a \"hidden layer\", just because is between the input and output layers, this is a dense layer which means is fully connected. The parameter 'units' is where we define the number of nodes that should be present in this hidden layers, the number is not optimal but a close approximation to the best one and it is a commin practice to use a power of 2, in this case 128. The activation function will be a recrifier function, the same as the convolutional layer.\n",
    "Now it is time to initialise our output layer, which should contain 6 nodes, because we have 6 classes. We will be using a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (3, 3),padding='same',input_shape = (480, 640, 1), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 5, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compile the model. We have to say that even though it says that is using a \"binary_crossentropy\" loss, this is not to worry, because we have 6 classes, so no binary, but the data is transformed in a way that this is the adequate loss function. If we use the \"categorical crossentropy\" we obtain bad results, because the data is transformed to work with the other one. We just wanted to say this is not a mistake, this is \"planned\". \n",
    "We can also chose the optimizer, in this case \"adam\" because we tried different ones but they are mostly the same.\n",
    "Then in \"metrics\" we chose to display the training accuracy and training recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy',tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make sure that the labels are correct, and categorically coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 5)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transform the data to comply with the expected input of the model, the \"reshape\" seems redundant, but we really resize the images earlier, this is just to sort the dimensions so the model will work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 480, 640\n",
    "train_data = train_data.reshape(train_data.shape[0], rows, cols, 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], rows, cols, 1)\n",
    "\n",
    "train_data = train_data.astype('float32')\n",
    "test_data = test_data.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure the reshape work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 480, 640, 1)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model, with 20 epochs. (\"verbose = 1\" is just to display the results as we are training)\n",
    "Here we want to clarify that we used all of the training images to actually train, because we tried other training methods using a percentatge of images as a validation, usually 10% or 20%, but because we have so few samples, only 60, this reduction on number of training images really affected the results. So is best to train will all the images and not with a validation set. We thought this will lead to overfit, but as the test results can demonstrate, this model is not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60/60 [==============================] - 146s 2s/step - loss: 10.4060 - accuracy: 0.6700 - recall_4: 0.3354\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 190s 3s/step - loss: 8.0232 - accuracy: 0.7933 - recall_4: 0.2328\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 210s 3s/step - loss: 3.1870 - accuracy: 0.7867 - recall_4: 0.4633\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 299s 5s/step - loss: 0.7070 - accuracy: 0.9233 - recall_4: 0.5898\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 332s 6s/step - loss: 0.4067 - accuracy: 0.9500 - recall_4: 0.6447\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 324s 5s/step - loss: 0.3019 - accuracy: 0.9633 - recall_4: 0.6761\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 303s 5s/step - loss: 0.1188 - accuracy: 0.9800 - recall_4: 0.7054\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 197s 3s/step - loss: 0.0312 - accuracy: 0.9967 - recall_4: 0.7391\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 251s 4s/step - loss: 0.0133 - accuracy: 0.9967 - recall_4: 0.7660\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 253s 4s/step - loss: 0.0045 - accuracy: 0.9967 - recall_4: 0.7900\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 192s 3s/step - loss: 0.0040 - accuracy: 1.0000 - recall_4: 0.8095\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 183s 3s/step - loss: 0.0019 - accuracy: 1.0000 - recall_4: 0.8257 \n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 234s 4s/step - loss: 6.4021e-04 - accuracy: 1.0000 - recall_4: 0.8394\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 228s 4s/step - loss: 2.9629e-04 - accuracy: 1.0000 - recall_4: 0.8510\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 165s 3s/step - loss: 2.3486e-04 - accuracy: 1.0000 - recall_4: 0.8611\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 195s 3s/step - loss: 1.8843e-04 - accuracy: 1.0000 - recall_4: 0.8700\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 176s 3s/step - loss: 1.4468e-04 - accuracy: 1.0000 - recall_4: 0.8777\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 143s 2s/step - loss: 1.2681e-04 - accuracy: 1.0000 - recall_4: 0.8846\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 166s 3s/step - loss: 9.1191e-05 - accuracy: 1.0000 - recall_4: 0.8907\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 176s 3s/step - loss: 7.1749e-05 - accuracy: 1.0000 - recall_4: 0.8963\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit(train_data, train_labels, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, with the model trained, is time to evaluate it. We do so with the \".evaluate\" function of the own model with the \"test_dataset\" and we get test loss, accuracy and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "45/45 [==============================] - 15s 331ms/step\n",
      "test loss, test acc, test recall: [5.784291606479221, 0.7644444704055786, 0.8776860237121582]\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results = classifier.evaluate(test_data, test_labels)\n",
    "print('test loss, test acc, test recall:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the F1-score as defined in the given project description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8171606702176178"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2*(results[1]*results[2]))/(results[1]+results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c6cae6b90>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test2_data[7],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(477, 548)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_data = test_data.reshape(test_data.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2,\n",
       "       3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4,\n",
       "       5])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(sd.VD_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels2 = np.zeros(len(sd.VD_LABEL))\n",
    "for it in range (0,len(sd.VD_LABEL)):\n",
    "    test_labels2[it] = sd.VD_LABEL[it]-1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
